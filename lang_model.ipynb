{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Opening the parsed Json File"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "with open('Z.json') as f:\n",
    "  data = json.load(f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data variable currently has a dictionary of key value pairs where the key represents the patois and the value represents the english translation\n",
    "Now, we're going to split the key value pairs into their respective lists\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "english = list(data.keys()) \n",
    "patois = list(data.values())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "questions_for_token = patois\n",
    "answers_for_token = english\n",
    "embed_size=100 #define the vector size based on word your embedding\n",
    "max_features=6000 #to restrict your number of unique words\n",
    "maxlen=100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Outlining anmd defining preprocessing functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import re\n",
    "def processTweet(chat):\n",
    "    \n",
    "    chat = re.sub(r'[\\.!:\\?\\-\\'\\\"\\\\/]', r'', chat)\n",
    "    chat = chat.strip('\\'\"')\n",
    "    return chat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def getFeatureVector(chat):\n",
    "    chat=processTweet(chat)\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = chat.split()\n",
    "    for w in words:\n",
    "        featureVector.append(w.lower())\n",
    "    return \" \".join(list(featureVector))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "woRD eMBEDDING USING gLOVE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def emb_mat(nb_words):\n",
    "    #may need to download the referenced file below\n",
    "    EMBEDDING_FILE=\"glove.6B.100d.txt\"\n",
    "    def get_coefs(word,*arr): \n",
    "        \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean,emb_std\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if (i >= max_features) or i==nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word) #here we will get embedding for each word from GloVe\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenization\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def tokenized_data(questions,answers,VOCAB_SIZE,tokenizer):\n",
    "    # encoder_input_data\n",
    "    import numpy as np\n",
    "    tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "    maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "    padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen , padding='post' )\n",
    "    encoder_input_data = np.array( padded_questions )\n",
    "    #print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "    # decoder_input_data\n",
    "    tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "    maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen , padding='post' )\n",
    "    decoder_input_data = np.array( padded_answers )\n",
    "    #print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "    # decoder_output_data\n",
    "    tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "    for i in range(len(tokenized_answers)) :\n",
    "        tokenized_answers[i] = tokenized_answers[i][1:] # remove <start> take rest\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen , padding='post' )\n",
    "    onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE)\n",
    "    decoder_output_data = np.array( onehot_answers )\n",
    "    #print( decoder_output_data.shape )\n",
    "    \n",
    "    return [encoder_input_data,decoder_input_data,decoder_output_data,maxlen_answers]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "DATA PREPARATION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "#define a savepoint for running the model\n",
    "filepath = \"model_Translate_new1.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "def prepare_data(questions,answers):\n",
    "    answers=pd.DataFrame(answers, columns=[\"Ans\"])\n",
    "    questions=pd.DataFrame(questions, columns=[\"Question\"])\n",
    "    questions[\"TokQues\"]=questions[\"Question\"].apply(getFeatureVector)\n",
    "\n",
    "    answers=np.array(answers[\"Ans\"])\n",
    "    questions=np.array(questions[\"TokQues\"])\n",
    "\n",
    "    answers_with_tags = list()\n",
    "    for i in range( len( answers ) ):\n",
    "        if type( answers[i] ) == str:\n",
    "            answers_with_tags.append( answers[i] )\n",
    "        else:\n",
    "            print(questions[i])\n",
    "            print(answers[i])\n",
    "            print(type(answers[i]))\n",
    "            questions.pop(i)\n",
    "\n",
    "    answers = list()\n",
    "    for i in range( len( answers_with_tags ) ) :\n",
    "        answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "    \n",
    "    \n",
    "    tokenizer = preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(questions+answers)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    #embedding_matrix=emb_mat(nb_words)[0]\n",
    "    #emb_vec=emb_mat(nb_words)[1]\n",
    "\n",
    "    VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "    \n",
    "    \n",
    "    tok_out=tokenized_data(questions,answers,VOCAB_SIZE,tokenizer)\n",
    "    encoder_input_data=tok_out[0]\n",
    "    decoder_input_data=tok_out[1]\n",
    "    decoder_output_data=tok_out[2]\n",
    "    maxlen_answers=tok_out[3]\n",
    "    \n",
    "    return [encoder_input_data,decoder_input_data,decoder_output_data,maxlen_answers,nb_words,word_index,tokenizer]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRAINING THE DATA\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "Prepared_data=prepare_data(questions_for_token,answers_for_token)\n",
    "encoder_input_data=Prepared_data[0]\n",
    "decoder_input_data=Prepared_data[1]\n",
    "decoder_output_data=Prepared_data[2]\n",
    "maxlen_answers=Prepared_data[3]\n",
    "nb_words=Prepared_data[4]\n",
    "word_index=Prepared_data[5]\n",
    "tokenizer=Prepared_data[6]\n",
    "embedding_matrix = emb_mat(nb_words)\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True, weights=[embedding_matrix]) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True,weights=[embedding_matrix]) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense( nb_words+1 , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=8, epochs=50, callbacks=callbacks_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "243/243 [==============================] - 61s 233ms/step - loss: 0.1591 - accuracy: 0.4851\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.15907, saving model to model_Translate_new1.h5\n",
      "Epoch 2/50\n",
      "243/243 [==============================] - 56s 232ms/step - loss: 0.1363 - accuracy: 0.5257\n",
      "\n",
      "Epoch 00002: loss improved from 0.15907 to 0.13628, saving model to model_Translate_new1.h5\n",
      "Epoch 3/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.1311 - accuracy: 0.5310\n",
      "\n",
      "Epoch 00003: loss improved from 0.13628 to 0.13111, saving model to model_Translate_new1.h5\n",
      "Epoch 4/50\n",
      "243/243 [==============================] - 57s 236ms/step - loss: 0.1264 - accuracy: 0.5373\n",
      "\n",
      "Epoch 00004: loss improved from 0.13111 to 0.12644, saving model to model_Translate_new1.h5\n",
      "Epoch 5/50\n",
      "243/243 [==============================] - 58s 237ms/step - loss: 0.1221 - accuracy: 0.5425\n",
      "\n",
      "Epoch 00005: loss improved from 0.12644 to 0.12214, saving model to model_Translate_new1.h5\n",
      "Epoch 6/50\n",
      "243/243 [==============================] - 56s 231ms/step - loss: 0.1178 - accuracy: 0.5490\n",
      "\n",
      "Epoch 00006: loss improved from 0.12214 to 0.11779, saving model to model_Translate_new1.h5\n",
      "Epoch 7/50\n",
      "243/243 [==============================] - 56s 232ms/step - loss: 0.1134 - accuracy: 0.5630\n",
      "\n",
      "Epoch 00007: loss improved from 0.11779 to 0.11336, saving model to model_Translate_new1.h5\n",
      "Epoch 8/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.1095 - accuracy: 0.5696\n",
      "\n",
      "Epoch 00008: loss improved from 0.11336 to 0.10952, saving model to model_Translate_new1.h5\n",
      "Epoch 9/50\n",
      "243/243 [==============================] - 55s 227ms/step - loss: 0.1055 - accuracy: 0.5828\n",
      "\n",
      "Epoch 00009: loss improved from 0.10952 to 0.10549, saving model to model_Translate_new1.h5\n",
      "Epoch 10/50\n",
      "243/243 [==============================] - 55s 226ms/step - loss: 0.1016 - accuracy: 0.5939\n",
      "\n",
      "Epoch 00010: loss improved from 0.10549 to 0.10156, saving model to model_Translate_new1.h5\n",
      "Epoch 11/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0980 - accuracy: 0.6090\n",
      "\n",
      "Epoch 00011: loss improved from 0.10156 to 0.09798, saving model to model_Translate_new1.h5\n",
      "Epoch 12/50\n",
      "243/243 [==============================] - 56s 231ms/step - loss: 0.0944 - accuracy: 0.6172\n",
      "\n",
      "Epoch 00012: loss improved from 0.09798 to 0.09437, saving model to model_Translate_new1.h5\n",
      "Epoch 13/50\n",
      "243/243 [==============================] - 57s 235ms/step - loss: 0.0912 - accuracy: 0.6318\n",
      "\n",
      "Epoch 00013: loss improved from 0.09437 to 0.09124, saving model to model_Translate_new1.h5\n",
      "Epoch 14/50\n",
      "243/243 [==============================] - 57s 235ms/step - loss: 0.0887 - accuracy: 0.6432\n",
      "\n",
      "Epoch 00014: loss improved from 0.09124 to 0.08866, saving model to model_Translate_new1.h5\n",
      "Epoch 15/50\n",
      "243/243 [==============================] - 56s 231ms/step - loss: 0.0855 - accuracy: 0.6531\n",
      "\n",
      "Epoch 00015: loss improved from 0.08866 to 0.08553, saving model to model_Translate_new1.h5\n",
      "Epoch 16/50\n",
      "243/243 [==============================] - 56s 231ms/step - loss: 0.0828 - accuracy: 0.6646\n",
      "\n",
      "Epoch 00016: loss improved from 0.08553 to 0.08284, saving model to model_Translate_new1.h5\n",
      "Epoch 17/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0804 - accuracy: 0.6729\n",
      "\n",
      "Epoch 00017: loss improved from 0.08284 to 0.08042, saving model to model_Translate_new1.h5\n",
      "Epoch 18/50\n",
      "243/243 [==============================] - 56s 232ms/step - loss: 0.0781 - accuracy: 0.6852\n",
      "\n",
      "Epoch 00018: loss improved from 0.08042 to 0.07813, saving model to model_Translate_new1.h5\n",
      "Epoch 19/50\n",
      "243/243 [==============================] - 57s 233ms/step - loss: 0.0765 - accuracy: 0.6861\n",
      "\n",
      "Epoch 00019: loss improved from 0.07813 to 0.07650, saving model to model_Translate_new1.h5\n",
      "Epoch 20/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0748 - accuracy: 0.6968\n",
      "\n",
      "Epoch 00020: loss improved from 0.07650 to 0.07479, saving model to model_Translate_new1.h5\n",
      "Epoch 21/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0730 - accuracy: 0.7045\n",
      "\n",
      "Epoch 00021: loss improved from 0.07479 to 0.07295, saving model to model_Translate_new1.h5\n",
      "Epoch 22/50\n",
      "243/243 [==============================] - 55s 228ms/step - loss: 0.0714 - accuracy: 0.7097\n",
      "\n",
      "Epoch 00022: loss improved from 0.07295 to 0.07144, saving model to model_Translate_new1.h5\n",
      "Epoch 23/50\n",
      "243/243 [==============================] - 55s 227ms/step - loss: 0.0702 - accuracy: 0.7173\n",
      "\n",
      "Epoch 00023: loss improved from 0.07144 to 0.07021, saving model to model_Translate_new1.h5\n",
      "Epoch 24/50\n",
      "243/243 [==============================] - 55s 228ms/step - loss: 0.0688 - accuracy: 0.7218\n",
      "\n",
      "Epoch 00024: loss improved from 0.07021 to 0.06884, saving model to model_Translate_new1.h5\n",
      "Epoch 25/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0675 - accuracy: 0.7288\n",
      "\n",
      "Epoch 00025: loss improved from 0.06884 to 0.06748, saving model to model_Translate_new1.h5\n",
      "Epoch 26/50\n",
      "243/243 [==============================] - 55s 227ms/step - loss: 0.0656 - accuracy: 0.7364\n",
      "\n",
      "Epoch 00026: loss improved from 0.06748 to 0.06558, saving model to model_Translate_new1.h5\n",
      "Epoch 27/50\n",
      "243/243 [==============================] - 56s 231ms/step - loss: 0.0642 - accuracy: 0.7435\n",
      "\n",
      "Epoch 00027: loss improved from 0.06558 to 0.06420, saving model to model_Translate_new1.h5\n",
      "Epoch 28/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0634 - accuracy: 0.7479\n",
      "\n",
      "Epoch 00028: loss improved from 0.06420 to 0.06338, saving model to model_Translate_new1.h5\n",
      "Epoch 29/50\n",
      "243/243 [==============================] - 55s 227ms/step - loss: 0.0625 - accuracy: 0.7490\n",
      "\n",
      "Epoch 00029: loss improved from 0.06338 to 0.06248, saving model to model_Translate_new1.h5\n",
      "Epoch 30/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0619 - accuracy: 0.7553\n",
      "\n",
      "Epoch 00030: loss improved from 0.06248 to 0.06192, saving model to model_Translate_new1.h5\n",
      "Epoch 31/50\n",
      "243/243 [==============================] - 57s 234ms/step - loss: 0.0614 - accuracy: 0.7599\n",
      "\n",
      "Epoch 00031: loss improved from 0.06192 to 0.06135, saving model to model_Translate_new1.h5\n",
      "Epoch 32/50\n",
      "243/243 [==============================] - 57s 233ms/step - loss: 0.0609 - accuracy: 0.7611\n",
      "\n",
      "Epoch 00032: loss improved from 0.06135 to 0.06090, saving model to model_Translate_new1.h5\n",
      "Epoch 33/50\n",
      "243/243 [==============================] - 56s 231ms/step - loss: 0.0605 - accuracy: 0.7644\n",
      "\n",
      "Epoch 00033: loss improved from 0.06090 to 0.06049, saving model to model_Translate_new1.h5\n",
      "Epoch 34/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0599 - accuracy: 0.7690\n",
      "\n",
      "Epoch 00034: loss improved from 0.06049 to 0.05994, saving model to model_Translate_new1.h5\n",
      "Epoch 35/50\n",
      "243/243 [==============================] - 56s 228ms/step - loss: 0.0596 - accuracy: 0.7720\n",
      "\n",
      "Epoch 00035: loss improved from 0.05994 to 0.05956, saving model to model_Translate_new1.h5\n",
      "Epoch 36/50\n",
      "243/243 [==============================] - 55s 227ms/step - loss: 0.0592 - accuracy: 0.7740\n",
      "\n",
      "Epoch 00036: loss improved from 0.05956 to 0.05921, saving model to model_Translate_new1.h5\n",
      "Epoch 37/50\n",
      "243/243 [==============================] - 55s 227ms/step - loss: 0.0588 - accuracy: 0.7765\n",
      "\n",
      "Epoch 00037: loss improved from 0.05921 to 0.05882, saving model to model_Translate_new1.h5\n",
      "Epoch 38/50\n",
      "243/243 [==============================] - 56s 231ms/step - loss: 0.0586 - accuracy: 0.7785\n",
      "\n",
      "Epoch 00038: loss improved from 0.05882 to 0.05857, saving model to model_Translate_new1.h5\n",
      "Epoch 39/50\n",
      "243/243 [==============================] - 56s 232ms/step - loss: 0.0580 - accuracy: 0.7808\n",
      "\n",
      "Epoch 00039: loss improved from 0.05857 to 0.05801, saving model to model_Translate_new1.h5\n",
      "Epoch 40/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0578 - accuracy: 0.7798\n",
      "\n",
      "Epoch 00040: loss improved from 0.05801 to 0.05775, saving model to model_Translate_new1.h5\n",
      "Epoch 41/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0575 - accuracy: 0.7839\n",
      "\n",
      "Epoch 00041: loss improved from 0.05775 to 0.05746, saving model to model_Translate_new1.h5\n",
      "Epoch 42/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0572 - accuracy: 0.7833\n",
      "\n",
      "Epoch 00042: loss improved from 0.05746 to 0.05723, saving model to model_Translate_new1.h5\n",
      "Epoch 43/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0568 - accuracy: 0.7856\n",
      "\n",
      "Epoch 00043: loss improved from 0.05723 to 0.05676, saving model to model_Translate_new1.h5\n",
      "Epoch 44/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0563 - accuracy: 0.7870\n",
      "\n",
      "Epoch 00044: loss improved from 0.05676 to 0.05631, saving model to model_Translate_new1.h5\n",
      "Epoch 45/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0556 - accuracy: 0.7908\n",
      "\n",
      "Epoch 00045: loss improved from 0.05631 to 0.05560, saving model to model_Translate_new1.h5\n",
      "Epoch 46/50\n",
      "243/243 [==============================] - 58s 239ms/step - loss: 0.0551 - accuracy: 0.7932\n",
      "\n",
      "Epoch 00046: loss improved from 0.05560 to 0.05514, saving model to model_Translate_new1.h5\n",
      "Epoch 47/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0549 - accuracy: 0.7928\n",
      "\n",
      "Epoch 00047: loss improved from 0.05514 to 0.05487, saving model to model_Translate_new1.h5\n",
      "Epoch 48/50\n",
      "243/243 [==============================] - 56s 230ms/step - loss: 0.0545 - accuracy: 0.7954\n",
      "\n",
      "Epoch 00048: loss improved from 0.05487 to 0.05452, saving model to model_Translate_new1.h5\n",
      "Epoch 49/50\n",
      "243/243 [==============================] - 56s 232ms/step - loss: 0.0543 - accuracy: 0.7960\n",
      "\n",
      "Epoch 00049: loss improved from 0.05452 to 0.05434, saving model to model_Translate_new1.h5\n",
      "Epoch 50/50\n",
      "243/243 [==============================] - 56s 229ms/step - loss: 0.0541 - accuracy: 0.7954\n",
      "\n",
      "Epoch 00050: loss improved from 0.05434 to 0.05414, saving model to model_Translate_new1.h5\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f923c05ca58>"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Making Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n",
    "\n",
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen , padding='post')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen , padding='post')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "enc_model , dec_model = make_inference_models()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def translate():\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "            empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "            stop_condition = False\n",
    "            decoded_translation = ''\n",
    "            while not stop_condition :\n",
    "                dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "                sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "                sampled_word = None\n",
    "                for word , index in tokenizer.word_index.items() :\n",
    "                    if sampled_word_index == index :\n",
    "                        decoded_translation += ' {}'.format( word )\n",
    "                        sampled_word = word\n",
    "                \n",
    "                if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "                    stop_condition = True\n",
    "                    \n",
    "                empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "                empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "                states_values = [ h , c ] \n",
    "\n",
    "            return( \" \".join(decoded_translation.strip().split(\" \")[:-1]) )\n",
    "        except:\n",
    "            print(\"(I don't understand).Try sumn else.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ah gud\n",
      "yuh a guh\n",
      "yuh a guh\n",
      "yuh a guh\n",
      "yuh a guh\n",
      "yuh a guh\n",
      "yuh a guh\n",
      "yuh a guh\n",
      "yuh a guh\n",
      "yuh a guh\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}